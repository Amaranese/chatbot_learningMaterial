{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP \n",
    "\n",
    "Python has some of the most extensive open-source NLP libraries, including the `Natural Language Toolkit or NLTK.`  \n",
    "https://www.nltk.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning \n",
    "\n",
    "`Noise removal` — stripping text of formatting (e.g., HTML tags).\n",
    "\n",
    "`Tokenization` — breaking text into individual words.  \n",
    "\n",
    "`Normalization` — cleaning text data in any other way:  \n",
    "\n",
    "`Stemming` is a blunt axe to chop off word prefixes and suffixes. “booing” and “booed” become “boo”, but “computer” may become “comput” and “are” would remain “are.”\n",
    "  \n",
    "`Lemmatization ` is a scalpel to bring words down to their root forms. For example, NLTK’s savvy lemmatizer knows “am” and “are” are related to “be.”\n",
    "Other common tasks include lowercasing, stopwords removal, spelling correction, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/adammcmurchie/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/adammcmurchie/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "Stemmed text:\n",
      "['So', 'mani', 'squid', 'are', 'jump', 'out', 'of', 'suitcas', 'these', 'day', 'that', 'you', 'can', 'bare', 'go', 'anywher', 'without', 'see', 'one', 'burst', 'forth', 'from', 'a', 'tightli', 'pack', 'valis', 'I', 'went', 'to', 'the', 'dentist', 'the', 'other', 'day', 'and', 'sure', 'enough', 'I', 'saw', 'an', 'angri', 'one', 'jump', 'out', 'of', 'my', 'dentist', 's', 'bag', 'within', 'minut', 'of', 'arriv', 'she', 'hardli', 'even', 'notic']\n",
      "\n",
      "Lemmatized text:\n",
      "['So', 'many', 'squid', 'are', 'jumping', 'out', 'of', 'suitcase', 'these', 'day', 'that', 'you', 'can', 'barely', 'go', 'anywhere', 'without', 'seeing', 'one', 'burst', 'forth', 'from', 'a', 'tightly', 'packed', 'valise', 'I', 'went', 'to', 'the', 'dentist', 'the', 'other', 'day', 'and', 'sure', 'enough', 'I', 'saw', 'an', 'angry', 'one', 'jump', 'out', 'of', 'my', 'dentist', 's', 'bag', 'within', 'minute', 'of', 'arriving', 'She', 'hardly', 'even', 'noticed']\n"
     ]
    }
   ],
   "source": [
    "# regex for removing punctuation!\n",
    "import re\n",
    "# nltk preprocessing magic\n",
    "import nltk\n",
    "nltk.download('punkt') # notebook only\n",
    "nltk.download('wordnet') # notebook only\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# grabbing a part of speech function:\n",
    "#from part_of_speech import get_part_of_speech\n",
    "\n",
    "text = \"So many squids are jumping out of suitcases these days that you can barely go anywhere without seeing one burst forth from a tightly packed valise. I went to the dentist the other day, and sure enough I saw an angry one jump out of my dentist's bag within minutes of arriving. She hardly even noticed.\"\n",
    "\n",
    "cleaned = re.sub('\\W+', ' ', text)\n",
    "tokenized = word_tokenize(cleaned)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = [stemmer.stem(token) for token in tokenized]\n",
    "\n",
    "## -- CHANGE these -- ##\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(x) for x in tokenized]\n",
    "\n",
    "print(\"Stemmed text:\")\n",
    "print(stemmed)\n",
    "print(\"\\nLemmatized text:\")\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are the lemmatized verbs like \"went\" still conjugated? By default `lemmatize()` treats every word as a noun.\n",
    "\n",
    "Give `lemmatize()` a second argument: `get_part_of_speech(token)` function added. This will tell our lemmatizer what part of speech the word is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed text:\n",
      "['So', 'mani', 'squid', 'are', 'jump', 'out', 'of', 'suitcas', 'these', 'day', 'that', 'you', 'can', 'bare', 'go', 'anywher', 'without', 'see', 'one', 'burst', 'forth', 'from', 'a', 'tightli', 'pack', 'valis', 'I', 'went', 'to', 'the', 'dentist', 'the', 'other', 'day', 'and', 'sure', 'enough', 'I', 'saw', 'an', 'angri', 'one', 'jump', 'out', 'of', 'my', 'dentist', 's', 'bag', 'within', 'minut', 'of', 'arriv', 'she', 'hardli', 'even', 'notic']\n",
      "\n",
      "Lemmatized text:\n",
      "['So', 'many', 'squid', 'be', 'jump', 'out', 'of', 'suitcase', 'these', 'day', 'that', 'you', 'can', 'barely', 'go', 'anywhere', 'without', 'see', 'one', 'burst', 'forth', 'from', 'a', 'tightly', 'pack', 'valise', 'I', 'go', 'to', 'the', 'dentist', 'the', 'other', 'day', 'and', 'sure', 'enough', 'I', 'saw', 'an', 'angry', 'one', 'jump', 'out', 'of', 'my', 'dentist', 's', 'bag', 'within', 'minute', 'of', 'arrive', 'She', 'hardly', 'even', 'notice']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "\n",
    "def get_part_of_speech(word):\n",
    "  probable_part_of_speech = wordnet.synsets(word)\n",
    "  pos_counts = Counter()\n",
    "  pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n",
    "  pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  )\n",
    "  pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  )\n",
    "  pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  )\n",
    "  \n",
    "  most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "  return most_likely_part_of_speech\n",
    "\n",
    "# regex for removing punctuation!\n",
    "import re\n",
    "# nltk preprocessing magic\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "text = \"So many squids are jumping out of suitcases these days that you can barely go anywhere without seeing one burst forth from a tightly packed valise. I went to the dentist the other day, and sure enough I saw an angry one jump out of my dentist's bag within minutes of arriving. She hardly even noticed.\"\n",
    "\n",
    "cleaned = re.sub('\\W+', ' ', text)\n",
    "tokenized = word_tokenize(cleaned)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = [stemmer.stem(token) for token in tokenized]\n",
    "\n",
    "## -- CHANGE these -- ##\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(x,get_part_of_speech(x)) for x in tokenized]\n",
    "\n",
    "print(\"Stemmed text:\")\n",
    "print(stemmed)\n",
    "print(\"\\nLemmatized text:\")\n",
    "print(lemmatized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parsing \n",
    "\n",
    "Parsing is an NLP process concerned with segmenting text based on syntax.  \n",
    "NLTK has a few tricks up its sleeve to help you out:    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "`Dependency grammar` trees help you understand the relationship between the words in a sentence. It can be a tedious task for a human, so the Python library spaCy is at your service, even if it isn’t always perfect.   \n",
    "\n",
    "`Regex parsing`, using Python’s ` re library`, allows for a bit more nuance. When coupled with `POS tagging`, you can identify specific phrase chunks. On its own, it can find you addresses, emails, and many other common patterns within large chunks of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part-of-speech tagging (POS tagging) \n",
    "\n",
    "identifies parts of speech (verbs, nouns, adjectives, etc.). NLTK can do it faster (and maybe more accurately) than your grammar teacher.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named entity recognition (NER)  \n",
    "helps identify the proper nouns (e.g., “Natalia” or “Berlin”) in a text. This can be a clue as to the topic of the text and NLTK captures many for you.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nltk tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        jumping                \n",
      "  _________|________________    \n",
      " |   |   squids    out      |  \n",
      " |   |     |        |       |   \n",
      " |   |    many      of     days\n",
      " |   |     |        |       |   \n",
      "are  .     So   suitcases these\n",
      "\n",
      "          go                       \n",
      "  ________|____________________     \n",
      " |   |    |       |      |  without\n",
      " |   |    |       |      |     |    \n",
      " |   |    |       |      |   seeing\n",
      " |   |    |       |      |     |    \n",
      "You can barely anywhere  .    one  \n",
      "\n",
      "          went               \n",
      "  _________|_________         \n",
      " |   |     to        |       \n",
      " |   |     |         |        \n",
      " |   |  dentist     day      \n",
      " |   |     |      ___|____    \n",
      " I   .    the   the     other\n",
      "\n",
      "                   saw                           \n",
      "  __________________|_________                    \n",
      " |   |   |    |              jump                \n",
      " |   |   |    |      _________|__________         \n",
      " |   |   |    |     |    |    |         out      \n",
      " |   |   |    |     |    |    |          |        \n",
      " |   |   |    |     |    |    |          of      \n",
      " |   |   |    |     |    |    |          |        \n",
      " |   |   |    |     |    |    |         bag      \n",
      " |   |   |    |     |    |    |          |        \n",
      " |   |   |  enough  |    |    |       dentist    \n",
      " |   |   |    |     |    |    |     _____|_____   \n",
      " ,   I   .   Sure   an angry one   my          's\n",
      "\n",
      "    noticed         \n",
      "  _____|__________   \n",
      "She  hardly even  . \n",
      "\n",
      "             Within                                                               \n",
      "  _____________|________________                                                   \n",
      " |                            vault                                               \n",
      " |    __________________________|________                                          \n",
      " |   |   |     |                        left                                      \n",
      " |   |   |     |       __________________|______                                   \n",
      " |   |   |     |      |    |    |              for                                \n",
      " |   |   |     |      |    |    |               |                                  \n",
      " |   |   |     |      |    |    |            millenia                             \n",
      " |   |   |     |      |    |    |     __________|_____________                     \n",
      " |   |   |     |      |    |    |    |                     stirred                \n",
      " |   |   |     |      |    |    |    |    ____________________|___________         \n",
      " |   |   |     |      |    |    |    |   |             |               waiting    \n",
      " |   |   |     |      |    |    |    |   |             |                  |        \n",
      " |   |   |     |      |    |    |    |   |            evil              awoken    \n",
      " |   |   |     |      |    |    |    |   |       ______|______       _____|_____   \n",
      " .  the old decrepid that had  been  a   ,      an         ancient  to          be\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "adding my sentence\n",
    "\"\"\"\n",
    "import spacy\n",
    "from nltk import Tree\n",
    "\n",
    "dependency_parser = spacy.load('en')\n",
    "squids_text = \"So many squids are jumping out of suitcases these days. You can barely go anywhere without seeing one. I went to the dentist the other day. Sure enough, I saw an angry one jump out of my dentist's bag. She hardly even noticed.\"\n",
    "parsed_squids = dependency_parser(squids_text)\n",
    "\n",
    "# Assign my_sentence a new value:\n",
    "my_sentence = \"Within the old decrepid vault that had been left for a millenia stirred an ancient evil, waiting to be awoken.\"\n",
    "my_parsed_sentence = dependency_parser(my_sentence)\n",
    "\n",
    "def to_nltk_tree(node):\n",
    "  if node.n_lefts + node.n_rights > 0:\n",
    "    parsed_child_nodes = [to_nltk_tree(child) for child in node.children]\n",
    "    return Tree(node.orth_, parsed_child_nodes)\n",
    "  else:\n",
    "    return node.orth_\n",
    "\n",
    "for sent in parsed_squids.sents:\n",
    "  to_nltk_tree(sent.root).pretty_print()\n",
    "  \n",
    "for sent in my_parsed_sentence.sents:\n",
    " to_nltk_tree(sent.root).pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bag of words \n",
    "\n",
    "The squids jumped out of the suitcases \n",
    "\n",
    "results in \n",
    "\n",
    "`{\"the\": 2, \"squid\": 1, \"jump\": 1, \"out\": 1, \"of\": 1, \"suitcase\": 1}`\n",
    "\n",
    "“Why are your suitcases full of jumping squids?”\n",
    "results in \n",
    "\n",
    "`{\"why\": 1, \"be\": 1, \"your\": 1, \"suitcase\": 1, \"full\": 1, \"of\": 1, \"jump\": 1, \"squid\": 1}`\n",
    "\n",
    "bag-of-words can be an excellent way of looking at language when you want to make predictions concerning topic or sentiment of a text. `When grammar and word order are irrelevant, this is probably a good model to use.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/adammcmurchie/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Counter({'humpty': 19, 'dumpty': 19, 'say': 19, 'alice': 16, 'name': 7, 'like': 7, 'think': 7, 'look': 6, 'im': 5, 'know': 5, 'mean': 5, 'go': 5, 'egg': 4, 'fall': 4, 'king': 4, 'would': 4, 'dont': 4, 'come': 3, 'write': 3, 'might': 3, 'sit': 3, 'one': 3, 'didnt': 3, 'take': 3, 'must': 3, 'stand': 3, 'hand': 3, 'remark': 3, 'never': 3, 'last': 3, 'wall': 3, 'horse': 3, 'men': 3, 'almost': 3, 'ask': 3, 'shape': 3, 'good': 3, 'another': 3, 'however': 2, 'large': 2, 'saw': 2, 'eye': 2, 'mouth': 2, 'cant': 2, 'face': 2, 'time': 2, 'narrow': 2, 'quite': 2, 'could': 2, 'long': 2, 'away': 2, 'speak': 2, 'call': 2, 'gently': 2, 'explain': 2, 'add': 2, 'turn': 2, 'conversation': 2, 'couldnt': 2, 'much': 2, 'interrupt': 2, 'course': 2, 'short': 2, 'laugh': 2, 'there': 2, 'cry': 2, 'make': 2, 'riddle': 2, 'thats': 2, 'behind': 2, 'book': 2, 'may': 2, 'ear': 2, 'little': 2, 'afraid': 2, 'old': 2, 'id': 2, 'get': 1, 'human': 1, 'within': 1, 'yard': 1, 'nose': 1, 'close': 1, 'clearly': 1, 'anybody': 1, 'else': 1, 'certain': 1, 'hundred': 1, 'easily': 1, 'enormous': 1, 'leg': 1, 'cross': 1, 'turk': 1, 'top': 1, 'high': 1, 'wallsuch': 1, 'wonder': 1, 'keep': 1, 'balanceand': 1, 'steadily': 1, 'fix': 1, 'opposite': 1, 'direction': 1, 'least': 1, 'notice': 1, 'stuff': 1, 'figure': 1, 'exactly': 1, 'aloud': 1, 'ready': 1, 'catch': 1, 'every': 1, 'moment': 1, 'expect': 1, 'provoke': 1, 'silence': 1, 'eggvery': 1, 'sir': 1, 'pretty': 1, 'hop': 1, 'sort': 1, 'compliment': 1, 'people': 1, 'usual': 1, 'sense': 1, 'baby': 1, 'wasnt': 1, 'anything': 1, 'fact': 1, 'evidently': 1, 'address': 1, 'treeso': 1, 'softly': 1, 'repeat': 1, 'great': 1, 'put': 1, 'place': 1, 'line': 1, 'poetry': 1, 'loud': 1, 'forget': 1, 'hear': 1, 'chatter': 1, 'first': 1, 'tell': 1, 'business': 1, 'stupid': 1, 'enough': 1, 'impatiently': 1, 'something': 1, 'doubtfully': 1, 'amand': 1, 'handsome': 1, 'alone': 1, 'wish': 1, 'begin': 1, 'argument': 1, 'nobody': 1, 'answer': 1, 'youd': 1, 'safe': 1, 'grind': 1, 'idea': 1, 'simply': 1, 'natured': 1, 'anxiety': 1, 'queer': 1, 'creature': 1, 'tremendously': 1, 'easy': 1, 'growl': 1, 'ever': 1, 'offwhich': 1, 'chance': 1, 'ofbut': 1, 'purse': 1, 'lip': 1, 'solemn': 1, 'grand': 1, 'hardly': 1, 'help': 1, 'promise': 1, 'mewith': 1, 'mouthtoto': 1, 'send': 1, 'rather': 1, 'unwisely': 1, 'declare': 1, 'bad': 1, 'break': 1, 'sudden': 1, 'passion': 1, 'youve': 1, 'listen': 1, 'doorsand': 1, 'treesand': 1, 'chimneysor': 1, 'havent': 1, 'indeed': 1, 'ah': 1, 'well': 1, 'thing': 1, 'calm': 1, 'tone': 1, 'history': 1, 'england': 1, 'mayhap': 1, 'youll': 1, 'see': 1, 'show': 1, 'proud': 1, 'shake': 1, 'grin': 1, 'lean': 1, 'forward': 1, 'nearly': 1, 'possible': 1, 'fell': 1, 'offer': 1, 'watch': 1, 'anxiously': 1, 'smile': 1, 'end': 1, 'meet': 1, 'happen': 1, 'head': 1, 'yes': 1, 'theyd': 1, 'pick': 1, 'minute': 1, 'fast': 1, 'let': 1, 'back': 1, 'remember': 1, 'politely': 1, 'case': 1, 'start': 1, 'fresh': 1, 'choose': 1, 'subject': 1, 'talk': 1, 'game': 1, 'here': 1, 'question': 1, 'calculation': 1, 'seven': 1, 'year': 1, 'six': 1, 'month': 1, 'wrong': 1, 'exclaim': 1, 'triumphantly': 1, 'word': 1, 'though': 1})\n"
     ]
    }
   ],
   "source": [
    "# importing regex and nltk\n",
    "import re, nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# importing Counter to get word counts for bag of words\n",
    "from collections import Counter\n",
    "# importing a passage from Through the Looking Glass\n",
    "looking_glass_text = \"\"\"\n",
    " However, the egg only got larger and larger, and more and more human: when she had come within a few yards of it, she saw that it had eyes and a nose and mouth; and when she had come close to it, she saw clearly that it was HUMPTY DUMPTY himself. It cant be anybody else! she said to herself. Im as certain of it, as if his name were written all over his face.\n",
    "\n",
    "It might have been written a hundred times, easily, on that enormous face. Humpty Dumpty was sitting with his legs crossed, like a Turk, on the top of a high wallsuch a narrow one that Alice quite wondered how he could keep his balanceand, as his eyes were steadily fixed in the opposite direction, and he didnt take the least notice of her, she thought he must be a stuffed figure after all.\n",
    "\n",
    "And how exactly like an egg he is! she said aloud, standing with her hands ready to catch him, for she was every moment expecting him to fall.\n",
    "\n",
    "Its very provoking, Humpty Dumpty said after a long silence, looking away from Alice as he spoke, to be called an eggVery!\n",
    "\n",
    "I said you looked like an egg, Sir, Alice gently explained. And some eggs are very pretty, you know she added, hoping to turn her remark into a sort of a compliment.\n",
    "\n",
    "Some people, said Humpty Dumpty, looking away from her as usual, have no more sense than a baby!\n",
    "\n",
    "Alice didnt know what to say to this: it wasnt at all like conversation, she thought, as he never said anything to her; in fact, his last remark was evidently addressed to a treeso she stood and softly repeated to herself:\n",
    "\n",
    "     Humpty Dumpty sat on a wall:\n",
    "     Humpty Dumpty had a great fall.\n",
    "     All the Kings horses and all the Kings men\n",
    "     Couldnt put Humpty Dumpty in his place again.\n",
    "\n",
    "That last line is much too long for the poetry, she added, almost out loud, forgetting that Humpty Dumpty would hear her.\n",
    "\n",
    "Dont stand there chattering to yourself like that, Humpty Dumpty said, looking at her for the first time, but tell me your name and your business.\n",
    "\n",
    "My name is Alice, but\n",
    "\n",
    "Its a stupid enough name! Humpty Dumpty interrupted impatiently. What does it mean?\n",
    "\n",
    "Must a name mean something? Alice asked doubtfully.\n",
    "\n",
    "Of course it must, Humpty Dumpty said with a short laugh: my name means the shape I amand a good handsome shape it is, too. With a name like yours, you might be any shape, almost.\n",
    "\n",
    "Why do you sit out here all alone? said Alice, not wishing to begin an argument.\n",
    "\n",
    "Why, because theres nobody with me! cried Humpty Dumpty. Did you think I didnt know the answer to that? Ask another.\n",
    "\n",
    "Dont you think youd be safer down on the ground? Alice went on, not with any idea of making another riddle, but simply in her good-natured anxiety for the queer creature. That wall is so very narrow!\n",
    "\n",
    "What tremendously easy riddles you ask! Humpty Dumpty growled out. Of course I dont think so! Why, if ever I did fall offwhich theres no chance ofbut if I did Here he pursed his lips and looked so solemn and grand that Alice could hardly help laughing. If I did fall, he went on, The King has promised mewith his very own mouthtoto\n",
    "\n",
    "To send all his horses and all his men, Alice interrupted, rather unwisely.\n",
    "\n",
    "Now I declare thats too bad! Humpty Dumpty cried, breaking into a sudden passion. Youve been listening at doorsand behind treesand down chimneysor you couldnt have known it!\n",
    "\n",
    "I havent, indeed! Alice said very gently. Its in a book.\n",
    "\n",
    "Ah, well! They may write such things in a book, Humpty Dumpty said in a calmer tone. Thats what you call a History of England, that is. Now, take a good look at me! Im one that has spoken to a King, I am: mayhap youll never see such another: and to show you Im not proud, you may shake hands with me! And he grinned almost from ear to ear, as he leant forwards (and as nearly as possible fell off the wall in doing so) and offered Alice his hand. She watched him a little anxiously as she took it. If he smiled much more, the ends of his mouth might meet behind, she thought: and then I dont know what would happen to his head! Im afraid it would come off!\n",
    "\n",
    "Yes, all his horses and all his men, Humpty Dumpty went on. Theyd pick me up again in a minute, they would! However, this conversation is going on a little too fast: lets go back to the last remark but one.\n",
    "\n",
    "Im afraid I cant quite remember it, Alice said very politely.\n",
    "\n",
    "In that case we start fresh, said Humpty Dumpty, and its my turn to choose a subject (He talks about it just as if it was a game! thought Alice.) So heres a question for you. How old did you say you were?\n",
    "\n",
    "Alice made a short calculation, and said Seven years and six months.\n",
    "\n",
    "Wrong! Humpty Dumpty exclaimed triumphantly. You never said a word like it!\n",
    "\n",
    "I though you meant How old are you? Alice explained.\n",
    "\n",
    "If Id meant that, Id have said it, said Humpty Dumpty. \n",
    "\"\"\"\n",
    "\n",
    "def get_part_of_speech(word):\n",
    "  probable_part_of_speech = wordnet.synsets(word)\n",
    "  pos_counts = Counter()\n",
    "  pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n",
    "  pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  )\n",
    "  pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  )\n",
    "  pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  )\n",
    "  \n",
    "  most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "  return most_likely_part_of_speech\n",
    "\n",
    "\n",
    "# importing part-of-speech function for lemmatization\n",
    "#from part_of_speech import get_part_of_speech\n",
    "# got from earlier function \n",
    "nltk.download('stopwords') # notebook only\n",
    "# Change text to another string:\n",
    "text = looking_glass_text\n",
    "\n",
    "cleaned = re.sub('\\W+', ' ', text).lower()\n",
    "tokenized = word_tokenize(cleaned)\n",
    "\n",
    "# Filter out stopwords ( i / me , him...)\n",
    "stop_words = stopwords.words('english')\n",
    "filtered = [word for word in tokenized if word not in stop_words]\n",
    "\n",
    "normalizer = WordNetLemmatizer()\n",
    "normalized = [normalizer.lemmatize(token, get_part_of_speech(token)) for token in filtered]\n",
    "\n",
    "# take the normalised words and bag em \n",
    "bag_of_looking_glass_words = Counter(normalized)\n",
    "print(bag_of_looking_glass_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Models: N-Gram and NLM\n",
    "For parsing entire phrases or conducting language prediction, you will want to use a model that pays attention to each word’s neighbors.  \n",
    "\n",
    "Unlike bag-of-words, the n-gram model considers` a sequence of some number (n) units `and calculates the `probability of each unit in a body of language` given the preceding sequence of length n. Because of this, n-gram probabilities with larger n values can be impressive at language prediction.\n",
    "    \n",
    "Take a look at our revised squid example:   \n",
    "`“The squids jumped out of the suitcases. The squids were furious.”`\n",
    "  \n",
    "A bigram model (where n is 2) might give us the following count frequencies:\n",
    "\n",
    "```\n",
    "{('', 'the'): 2, ('the', 'squids'): 2, ('squids', 'jumped'): 1, ('jumped', 'out'): 1, ('out', 'of'): 1, ('of', 'the'): 1, ('the', 'suitcases'): 1, ('suitcases', ''): 1, ('squids', 'were'): 1, ('were', 'furious'): 1, ('furious', ''): 1}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple problems with the n gram model:  \n",
    "   \n",
    "How can your language model make sense of the sentence “The cat fell asleep in the mailbox” if it’s never seen the word “mailbox” before? During training, your model will probably come across test words that it has never encountered before (this issue also pertains to bag of words). A tactic known as language smoothing can help adjust probabilities for unknown words, but it isn’t always ideal.   \n",
    "  \n",
    "For a model that more accurately predicts human language patterns, you want n (your sequence length) to be as large as possible. That way, you will have more natural sounding language, right? Well, as the sequence length grows, the number of examples of each sequence within your training corpus shrinks. With too few examples, you won’t have enough data to make many predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter `neural language models (NLMs)`! Much recent work within NLP has involved developing and training neural networks to approximate the approach our human brains take towards language. This deep learning approach allows computers a much more adaptive tack to processing human language. Common NLMs include LSTMs and transformer models.\n",
    "\n",
    "## Finding the most common phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking Glass Bigrams:\n",
      "[(('of', 'the'), 101), (('said', 'the'), 98), (('in', 'a'), 97), (('in', 'the'), 90), (('as', 'she'), 82), (('you', 'know'), 72), (('a', 'little'), 68), (('the', 'queen'), 67), (('said', 'alice'), 67), (('to', 'the'), 66)]\n",
      "\n",
      "Looking Glass Trigrams:\n",
      "[(('the', 'red', 'queen'), 54), (('the', 'white', 'queen'), 31), (('said', 'in', 'a'), 21), (('she', 'went', 'on'), 18), (('said', 'the', 'red'), 17), (('thought', 'to', 'herself'), 16), (('the', 'queen', 'said'), 16), (('said', 'to', 'herself'), 14), (('said', 'humpty', 'dumpty'), 14), (('the', 'knight', 'said'), 14)]\n",
      "\n",
      "Looking Glass n-grams:\n",
      "[(('the', 'red', 'queen'), 54), (('the', 'white', 'queen'), 31), (('said', 'in', 'a'), 21), (('she', 'went', 'on'), 18), (('said', 'the', 'red'), 17), (('thought', 'to', 'herself'), 16), (('the', 'queen', 'said'), 16), (('said', 'to', 'herself'), 14), (('said', 'humpty', 'dumpty'), 14), (('the', 'knight', 'said'), 14)]\n"
     ]
    }
   ],
   "source": [
    "import nltk, re\n",
    "from nltk.tokenize import word_tokenize\n",
    "# importing ngrams module from nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "from looking_glass import looking_glass_full_text # held locally\n",
    "\n",
    "# clean and tokenise\n",
    "cleaned = re.sub('\\W+', ' ', looking_glass_full_text).lower()\n",
    "tokenized = word_tokenize(cleaned)\n",
    "\n",
    "# Change the n value to 2:\n",
    "looking_glass_bigrams = ngrams(tokenized, 2)\n",
    "looking_glass_bigrams_frequency = Counter(looking_glass_bigrams)\n",
    "\n",
    "# Change the n value to 3:\n",
    "looking_glass_trigrams = ngrams(tokenized, 3)\n",
    "looking_glass_trigrams_frequency = Counter(looking_glass_trigrams)\n",
    "\n",
    "# Change the n value to a number greater than 3:\n",
    "looking_glass_ngrams = ngrams(tokenized, 3)\n",
    "looking_glass_ngrams_frequency = Counter(looking_glass_ngrams)\n",
    "\n",
    "print(\"Looking Glass Bigrams:\")\n",
    "print(looking_glass_bigrams_frequency.most_common(10))\n",
    "\n",
    "print(\"\\nLooking Glass Trigrams:\")\n",
    "print(looking_glass_trigrams_frequency.most_common(10))\n",
    "\n",
    "print(\"\\nLooking Glass n-grams:\")\n",
    "print(looking_glass_ngrams_frequency.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: At what n are you just getting lines from poems repeated in the text? This is where there may be too few examples of each sequence within your training corpus to make any helpful predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic Models\n",
    "We’ve touched on the idea of finding topics within a body of language. But what if the text is long and the topics aren’t obvious?  \n",
    "\n",
    "Topic modeling is an area of NLP dedicated to uncovering latent, or hidden, topics within a body of language. For example, one Codecademy curriculum developer used topic modeling to discover patterns within Taylor Swift songs related to love and heartbreak over time.  \n",
    "https://www.codecademy.com/resources/blog/taylor-swift-lyrics-machine-learning/\n",
    "  \n",
    "\n",
    "A common technique is to deprioritize the most common words and prioritize less frequently used terms as topics in a process known as `term frequency-inverse document frequency (tf-idf)`. Say what?! This may sound counter-intuitive at first. Why would you want to give more priority to less-used words? Well, when you’re working with a lot of text, it makes a bit of sense if you don’t want your topics filled with words like “the” and “is.” The Python libraries gensim and sklearn have modules to handle tf-idf.  \n",
    "  \n",
    "Whether you use your plain bag of words (which will give you term frequency) or run it through `tf-idf`, the next step in your topic modeling journey is often `latent Dirichlet allocation (LDA)`. LDA is a statistical model that takes your documents and determines which words `keep popping up together in the same contexts` (i.e., documents). We’ll use sklearn to tackle this for us.  \n",
    "\n",
    "If you have any interest in `visualizing your newly minted topics, word2vec is a great technique to have up your sleeve`. word2vec can map out your topic model results spatially as vectors so that similarly used words are closer together. In the case of a language sample consisting of “The squids jumped out of the suitcases. The squids were furious. Why are your suitcases full of jumping squids?”, we might see that “suitcase”, “jump”, and “squid” were words used within similar contexts. This word-to-vector mapping is known as a word embedding.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~ Topics found by bag of words LDA ~~~\n",
      "Topic #1: say could one look\n",
      "Topic #2: holmes see say know\n",
      "Topic #3: holmes greet precaution would\n",
      "Topic #4: look narrow time come\n",
      "Topic #5: holmes say upon know\n",
      "Topic #6: say upon holmes man\n",
      "Topic #7: man say one see\n",
      "Topic #8: holmes upon say know\n",
      "Topic #9: holmes know man say\n",
      "Topic #10: holmes say would know\n",
      "\n",
      "\n",
      "~~~ Topics found by tf-idf LDA ~~~\n",
      "Topic #1: year scarlet clutch waylay\n",
      "Topic #2: confession ha six stair\n",
      "Topic #3: broad sherlock grass dark\n",
      "Topic #4: year waylay boy leg\n",
      "Topic #5: cry innocent consider dear\n",
      "Topic #6: majesty several inquest official\n",
      "Topic #7: bristol client bedroom moment\n",
      "Topic #8: holmes say know upon\n",
      "Topic #9: regent ransack suspicious flush\n",
      "Topic #10: day quarrel assize secure\n"
     ]
    }
   ],
   "source": [
    "import nltk, re\n",
    "from sherlock_holmes import bohemia_ch1, bohemia_ch2, bohemia_ch3, boscombe_ch1, boscombe_ch2, boscombe_ch3\n",
    "\n",
    "import nltk, re\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "normalizer = WordNetLemmatizer()\n",
    "\n",
    "def get_part_of_speech(word):\n",
    "  probable_part_of_speech = wordnet.synsets(word)\n",
    "  pos_counts = Counter()\n",
    "  pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n",
    "  pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  )\n",
    "  pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  )\n",
    "  pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  )\n",
    "  most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "  return most_likely_part_of_speech\n",
    "\n",
    "def preprocess_text(text):\n",
    "  cleaned = re.sub(r'\\W+', ' ', text).lower()\n",
    "  tokenized = word_tokenize(cleaned)\n",
    "  normalized = [normalizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized]\n",
    "  filtered = [word for word in normalized if word not in stop_words]\n",
    "  return \" \".join(filtered)\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# preparing the text\n",
    "corpus = [bohemia_ch1, bohemia_ch2, bohemia_ch3, boscombe_ch1, boscombe_ch2, boscombe_ch3]\n",
    "preprocessed_corpus = [preprocess_text(chapter) for chapter in corpus]\n",
    "\n",
    "# UPDATE STOP LIST\n",
    "stop_list = ['say','see','shall','it','the','then','here','there','where','and','is','upon','one']\n",
    "# filtering topics for stop words\n",
    "def filter_out_stop_words(corpus):\n",
    "  no_stops_corpus = []\n",
    "  for chapter in corpus:\n",
    "    no_stops_chapter = \" \".join([word for word in chapter.split(\" \") if word not in stop_list])\n",
    "    no_stops_corpus.append(no_stops_chapter)\n",
    "  return no_stops_corpus\n",
    "filtered_for_stops = filter_out_stop_words(preprocessed_corpus)\n",
    "\n",
    "# creating the bag of words model\n",
    "bag_of_words_creator = CountVectorizer()\n",
    "bag_of_words = bag_of_words_creator.fit_transform(filtered_for_stops)\n",
    "\n",
    "# creating the tf-idf model\n",
    "tfidf_creator = TfidfVectorizer(min_df = 0.2)\n",
    "tfidf = tfidf_creator.fit_transform(preprocessed_corpus)\n",
    "\n",
    "# creating the bag of words LDA model\n",
    "lda_bag_of_words_creator = LatentDirichletAllocation(learning_method='online', n_components=10)\n",
    "lda_bag_of_words = lda_bag_of_words_creator.fit_transform(bag_of_words)\n",
    "\n",
    "# creating the tf-idf LDA model\n",
    "lda_tfidf_creator = LatentDirichletAllocation(learning_method='online', n_components=10)\n",
    "lda_tfidf = lda_tfidf_creator.fit_transform(tfidf)\n",
    "\n",
    "print(\"~~~ Topics found by bag of words LDA ~~~\")\n",
    "for topic_id, topic in enumerate(lda_bag_of_words_creator.components_):\n",
    "  message = \"Topic #{}: \".format(topic_id + 1)\n",
    "  message += \" \".join([bag_of_words_creator.get_feature_names()[i] for i in topic.argsort()[:-5 :-1]])\n",
    "  print(message)\n",
    "\n",
    "print(\"\\n\\n~~~ Topics found by tf-idf LDA ~~~\")\n",
    "for topic_id, topic in enumerate(lda_tfidf_creator.components_):\n",
    "  message = \"Topic #{}: \".format(topic_id + 1)\n",
    "  message += \" \".join([tfidf_creator.get_feature_names()[i] for i in topic.argsort()[:-5 :-1]])\n",
    "  print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Similarity (Levenshtein distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Addressing word similarity and misspelling for spellcheck or autocorrect often involves considering the `Levenshtein distance` or minimal edit distance between two words.  \n",
    "The distance is calculated through the minimum number of insertions, deletions, and substitutions that would need to occur for one word to become another. For example, turning “bees” into “beans” would require one substitution (“a” for “e”) and one insertion (“n”), so the Levenshtein distance would be two.  \n",
    "  \n",
    "Phonetic similarity is also a major challenge within speech recognition. English-speaking humans can easily tell from context whether someone said “euthanasia” or “youth in Asia,” but it’s a far more challenging task for a machine! More advanced autocorrect and spelling correction technology additionally considers key distance on a keyboard and phonetic similarity (how much two words or phrases sound the same).  \n",
    "  \n",
    "It’s also helpful to find out if texts are the same to guard against plagiarism, which we can identify through lexical similarity (the degree to which texts use the same vocabulary and phrases). Meanwhile, semantic similarity (the degree to which documents contain similar meaning or topics) is useful when you want to find (or recommend) an article or book similar to one you recently finished.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Levenshtein distance from 'fart' to 'target' is 3!\n",
      "The Levenshtein distance from 'code' to 'post' is 3!\n",
      "The Levenshtein distance from 'chunk' to 'trunk' is 2!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "# NLTK has a built-in function\n",
    "# to check Levenshtein distance:\n",
    "from nltk.metrics import edit_distance\n",
    "\n",
    "def print_levenshtein(string1, string2):\n",
    "  print(\"The Levenshtein distance from '{0}' to '{1}' is {2}!\".format(string1, string2, edit_distance(string1, string2)))\n",
    "\n",
    "# Check the distance between\n",
    "# any two words here!\n",
    "print_levenshtein(\"fart\", \"target\")\n",
    "\n",
    "# Assign passing strings here:\n",
    "three_away_from_code = \"post\"\n",
    "\n",
    "two_away_from_chunk = \"trunk\"\n",
    "\n",
    "print_levenshtein(\"code\", three_away_from_code)\n",
    "print_levenshtein(\"chunk\", two_away_from_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Prediction & Text Generation \n",
    "  \n",
    "How does your favorite search engine complete your search queries? How does your phone’s keyboard know what you want to type next? Language prediction is an application of NLP concerned with predicting text given preceding text. Autosuggest, autocomplete, and suggested replies are common forms of language prediction.  \n",
    "  \n",
    "Your first step to language prediction is picking a language model. `Bag of words alone is generally not a great model for language prediction; no matter what the preceding word was, you will just get one of the most commonly used words from your training corpus. ` \n",
    "  \n",
    "If you go the `n-gram` route, you will most likely rely on `Markov chains` to predict the statistical likelihood of each following word (or character) based on the training corpus. Markov chains are memory-less and make statistical predictions based entirely on the current n-gram on hand.  \n",
    "  \n",
    "For example, let’s take a sentence beginning, “I ate so many grilled cheese”. Using a trigram model (where n is 3), a Markov chain would predict the following word as “sandwiches” based on the number of times the sequence “grilled cheese sandwiches” has appeared in the training data out of all the times “grilled cheese” has appeared in the training data.  \n",
    "  \n",
    "A more advanced approach, using a neural language model, is the `Long Short Term Memory (LSTM) model`. LSTM uses deep learning with a network of artificial `“cells”` that manage memory, making them better suited for text prediction than traditional neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the galaxy will last two hundred billion years another wildly about at the same not designed to do not you to the sake of his thinning hair as to this immortality the universe will say the galactic council i didn t do s0 i didn t know said good point\n"
     ]
    }
   ],
   "source": [
    "import nltk, re, random\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict, deque\n",
    "from resources.td1 import training_doc1\n",
    "from resources.td2 import training_doc2\n",
    "from resources.td3 import training_doc3\n",
    "\n",
    "class MarkovChain:\n",
    "  def __init__(self):\n",
    "    self.lookup_dict = defaultdict(list)\n",
    "    self._seeded = False\n",
    "    self.__seed_me()\n",
    "\n",
    "  def __seed_me(self, rand_seed=None):\n",
    "    if self._seeded is not True:\n",
    "      try:\n",
    "        if rand_seed is not None:\n",
    "          random.seed(rand_seed)\n",
    "        else:\n",
    "          random.seed()\n",
    "        self._seeded = True\n",
    "      except NotImplementedError:\n",
    "        self._seeded = False\n",
    "    \n",
    "  def add_document(self, str):\n",
    "    preprocessed_list = self._preprocess(str)\n",
    "    pairs = self.__generate_tuple_keys(preprocessed_list)\n",
    "    for pair in pairs:\n",
    "      self.lookup_dict[pair[0]].append(pair[1])\n",
    "  \n",
    "  def _preprocess(self, str):\n",
    "    cleaned = re.sub(r'\\W+', ' ', str).lower()\n",
    "    tokenized = word_tokenize(cleaned)\n",
    "    return tokenized\n",
    "\n",
    "  def __generate_tuple_keys(self, data):\n",
    "    if len(data) < 1:\n",
    "      return\n",
    "\n",
    "    for i in range(len(data) - 1):\n",
    "      yield [ data[i], data[i + 1] ]\n",
    "      \n",
    "  def generate_text(self, max_length=50):\n",
    "    context = deque()\n",
    "    output = []\n",
    "    if len(self.lookup_dict) > 0:\n",
    "      self.__seed_me(rand_seed=len(self.lookup_dict))\n",
    "      chain_head = [list(self.lookup_dict)[0]]\n",
    "      context.extend(chain_head)\n",
    "      \n",
    "      while len(output) < (max_length - 1):\n",
    "        next_choices = self.lookup_dict[context[-1]]\n",
    "        if len(next_choices) > 0:\n",
    "          next_word = random.choice(next_choices)\n",
    "          context.append(next_word)\n",
    "          output.append(context.popleft())\n",
    "        else:\n",
    "          break\n",
    "      output.extend(list(context))\n",
    "    return \" \".join(output)\n",
    "\n",
    "my_markov = MarkovChain()\n",
    "my_markov.add_document(training_doc1)\n",
    "my_markov.add_document(training_doc2)\n",
    "my_markov.add_document(training_doc3)\n",
    "generated_text = my_markov.generate_text()\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced NLP Topics Naive Bayes \n",
    "\n",
    "  \n",
    "Believe it or not, you’ve just scratched the surface of natural language processing. There are a slew of advanced topics and applications of NLP, many of which rely on deep learning and neural networks.\n",
    "  \n",
    "`Naive Bayes classifiers` are supervised machine learning algorithms that leverage a probabilistic theorem to make predictions and classifications. They are widely used for sentiment analysis (determining whether a given block of language expresses negative or positive feelings) and spam filtering.\n",
    "    \n",
    "We’ve made enormous gains in `machine translation`, but even the most advanced translation software using neural networks and LSTM still has far to go in accurately translating between languages.\n",
    "\n",
    "Some of the most life-altering applications of NLP are focused on improving `language accessibility` for people with disabilities. Text-to-speech functionality and speech recognition have improved rapidly thanks to neural language models, making digital spaces far more accessible places.\n",
    "\n",
    "NLP can also be used to detect bias in writing and speech. Feel like a political candidate, book, or news source is biased but can’t put your finger on exactly how? Natural language processing can help you identify the language at issue.  \n",
    "  \n",
    "https://medium.com/agatha-codes/a-bossy-sort-of-voice-3c3a18de3093"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'reviews'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-77e61f35485e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mreviews\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcounter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_counts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Add your review:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'reviews'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "counter          = pickle.load( open( \"count_vect.p\", \"rb\" ) )\n",
    "training_counts =  pickle.load( open( \"train.p\", \"rb\" ) )\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Add your review:\n",
    "review = \"I have really enjoyed the course material and the interface is ok, but could be better. Overall I think this is a good course.\"\n",
    "review_counts = counter.transform([review])\n",
    "\n",
    "classifier = MultinomialNB()\n",
    "training_labels = [0] * 1000 + [1] * 1000\n",
    "\n",
    "classifier.fit(training_counts, training_labels)\n",
    "  \n",
    "neg = (classifier.predict_proba(review_counts)[0][0] * 100).round()\n",
    "pos = (classifier.predict_proba(review_counts)[0][1] * 100).round()\n",
    "\n",
    "if pos > 50:\n",
    "  print(\"Thank you for your positive review!\")\n",
    "elif neg > 50:\n",
    "  print(\"We're sorry this hasn't been the best possible lesson for you! We're always looking to improve.\")\n",
    "else:\n",
    "  print(\"Naive Bayes cannot determine if this is negative or positive. Thank you or we're sorry?\")\n",
    "\n",
    "  \n",
    "print(\"\\nAccording to our trained Naive Bayes classifier, the probability that your review was negative was {0}% and the probability it was positive was {1}%.\".format(neg, pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
