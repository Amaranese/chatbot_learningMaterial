{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP \n",
    "\n",
    "Python has some of the most extensive open-source NLP libraries, including the `Natural Language Toolkit or NLTK.`  \n",
    "https://www.nltk.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning \n",
    "\n",
    "`Noise removal` — stripping text of formatting (e.g., HTML tags).\n",
    "\n",
    "`Tokenization` — breaking text into individual words.  \n",
    "\n",
    "`Normalization` — cleaning text data in any other way:  \n",
    "\n",
    "`Stemming` is a blunt axe to chop off word prefixes and suffixes. “booing” and “booed” become “boo”, but “computer” may become “comput” and “are” would remain “are.”\n",
    "  \n",
    "`Lemmatization ` is a scalpel to bring words down to their root forms. For example, NLTK’s savvy lemmatizer knows “am” and “are” are related to “be.”\n",
    "Other common tasks include lowercasing, stopwords removal, spelling correction, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/adammcmurchie/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/adammcmurchie/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed text:\n",
      "['So', 'mani', 'squid', 'are', 'jump', 'out', 'of', 'suitcas', 'these', 'day', 'that', 'you', 'can', 'bare', 'go', 'anywher', 'without', 'see', 'one', 'burst', 'forth', 'from', 'a', 'tightli', 'pack', 'valis', 'I', 'went', 'to', 'the', 'dentist', 'the', 'other', 'day', 'and', 'sure', 'enough', 'I', 'saw', 'an', 'angri', 'one', 'jump', 'out', 'of', 'my', 'dentist', 's', 'bag', 'within', 'minut', 'of', 'arriv', 'she', 'hardli', 'even', 'notic']\n",
      "\n",
      "Lemmatized text:\n",
      "['So', 'many', 'squid', 'are', 'jumping', 'out', 'of', 'suitcase', 'these', 'day', 'that', 'you', 'can', 'barely', 'go', 'anywhere', 'without', 'seeing', 'one', 'burst', 'forth', 'from', 'a', 'tightly', 'packed', 'valise', 'I', 'went', 'to', 'the', 'dentist', 'the', 'other', 'day', 'and', 'sure', 'enough', 'I', 'saw', 'an', 'angry', 'one', 'jump', 'out', 'of', 'my', 'dentist', 's', 'bag', 'within', 'minute', 'of', 'arriving', 'She', 'hardly', 'even', 'noticed']\n"
     ]
    }
   ],
   "source": [
    "# regex for removing punctuation!\n",
    "import re\n",
    "# nltk preprocessing magic\n",
    "import nltk\n",
    "nltk.download('punkt') # notebook only\n",
    "nltk.download('wordnet') # notebook only\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# grabbing a part of speech function:\n",
    "#from part_of_speech import get_part_of_speech\n",
    "\n",
    "text = \"So many squids are jumping out of suitcases these days that you can barely go anywhere without seeing one burst forth from a tightly packed valise. I went to the dentist the other day, and sure enough I saw an angry one jump out of my dentist's bag within minutes of arriving. She hardly even noticed.\"\n",
    "\n",
    "cleaned = re.sub('\\W+', ' ', text)\n",
    "tokenized = word_tokenize(cleaned)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = [stemmer.stem(token) for token in tokenized]\n",
    "\n",
    "## -- CHANGE these -- ##\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(x) for x in tokenized]\n",
    "\n",
    "print(\"Stemmed text:\")\n",
    "print(stemmed)\n",
    "print(\"\\nLemmatized text:\")\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are the lemmatized verbs like \"went\" still conjugated? By default `lemmatize()` treats every word as a noun.\n",
    "\n",
    "Give `lemmatize()` a second argument: `get_part_of_speech(token)` function added. This will tell our lemmatizer what part of speech the word is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed text:\n",
      "['So', 'mani', 'squid', 'are', 'jump', 'out', 'of', 'suitcas', 'these', 'day', 'that', 'you', 'can', 'bare', 'go', 'anywher', 'without', 'see', 'one', 'burst', 'forth', 'from', 'a', 'tightli', 'pack', 'valis', 'I', 'went', 'to', 'the', 'dentist', 'the', 'other', 'day', 'and', 'sure', 'enough', 'I', 'saw', 'an', 'angri', 'one', 'jump', 'out', 'of', 'my', 'dentist', 's', 'bag', 'within', 'minut', 'of', 'arriv', 'she', 'hardli', 'even', 'notic']\n",
      "\n",
      "Lemmatized text:\n",
      "['So', 'many', 'squid', 'be', 'jump', 'out', 'of', 'suitcase', 'these', 'day', 'that', 'you', 'can', 'barely', 'go', 'anywhere', 'without', 'see', 'one', 'burst', 'forth', 'from', 'a', 'tightly', 'pack', 'valise', 'I', 'go', 'to', 'the', 'dentist', 'the', 'other', 'day', 'and', 'sure', 'enough', 'I', 'saw', 'an', 'angry', 'one', 'jump', 'out', 'of', 'my', 'dentist', 's', 'bag', 'within', 'minute', 'of', 'arrive', 'She', 'hardly', 'even', 'notice']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_part_of_speech(word):\n",
    "  probable_part_of_speech = wordnet.synsets(word)\n",
    "  pos_counts = Counter()\n",
    "  pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n",
    "  pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  )\n",
    "  pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  )\n",
    "  pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  )\n",
    "  \n",
    "  most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "  return most_likely_part_of_speech\n",
    "\n",
    "# regex for removing punctuation!\n",
    "import re\n",
    "# nltk preprocessing magic\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "text = \"So many squids are jumping out of suitcases these days that you can barely go anywhere without seeing one burst forth from a tightly packed valise. I went to the dentist the other day, and sure enough I saw an angry one jump out of my dentist's bag within minutes of arriving. She hardly even noticed.\"\n",
    "\n",
    "cleaned = re.sub('\\W+', ' ', text)\n",
    "tokenized = word_tokenize(cleaned)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = [stemmer.stem(token) for token in tokenized]\n",
    "\n",
    "## -- CHANGE these -- ##\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(x,get_part_of_speech(x)) for x in tokenized]\n",
    "\n",
    "print(\"Stemmed text:\")\n",
    "print(stemmed)\n",
    "print(\"\\nLemmatized text:\")\n",
    "print(lemmatized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed text:\n",
      "['So', 'mani', 'squid', 'are', 'jump', 'out', 'of', 'suitcas', 'these', 'day', 'that', 'you', 'can', 'bare', 'go', 'anywher', 'without', 'see', 'one', 'burst', 'forth', 'from', 'a', 'tightli', 'pack', 'valis', 'I', 'went', 'to', 'the', 'dentist', 'the', 'other', 'day', 'and', 'sure', 'enough', 'I', 'saw', 'an', 'angri', 'one', 'jump', 'out', 'of', 'my', 'dentist', 's', 'bag', 'within', 'minut', 'of', 'arriv', 'she', 'hardli', 'even', 'notic']\n",
      "\n",
      "Lemmatized text:\n",
      "['So', 'many', 'squid', 'are', 'jumping', 'out', 'of', 'suitcase', 'these', 'day', 'that', 'you', 'can', 'barely', 'go', 'anywhere', 'without', 'seeing', 'one', 'burst', 'forth', 'from', 'a', 'tightly', 'packed', 'valise', 'I', 'went', 'to', 'the', 'dentist', 'the', 'other', 'day', 'and', 'sure', 'enough', 'I', 'saw', 'an', 'angry', 'one', 'jump', 'out', 'of', 'my', 'dentist', 's', 'bag', 'within', 'minute', 'of', 'arriving', 'She', 'hardly', 'even', 'noticed']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parsing \n",
    "\n",
    "Parsing is an NLP process concerned with segmenting text based on syntax.  \n",
    "NLTK has a few tricks up its sleeve to help you out:   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-of-speech tagging (POS tagging) \n",
    "\n",
    "identifies parts of speech (verbs, nouns, adjectives, etc.). NLTK can do it faster (and maybe more accurately) than your grammar teacher.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named entity recognition (NER) \n",
    " \n",
    "helps identify the proper nouns (e.g., “Natalia” or “Berlin”) in a text. This can be a clue as to the topic of the text and NLTK captures many for you.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency grammar \n",
    "trees help you understand the relationship between the words in a sentence. It can be a tedious task for a human, so the Python library spaCy is at your service, even if it isn’t always perfect.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regex parsing \n",
    "using Python’s ` re library`, allows for a bit more nuance. When coupled with `POS tagging`, you can identify specific phrase chunks. On its own, it can find you addresses, emails, and many other common patterns within large chunks of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nltk tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        jumping                \n",
      "  _________|________________    \n",
      " |   |   squids    out      |  \n",
      " |   |     |        |       |   \n",
      " |   |    many      of     days\n",
      " |   |     |        |       |   \n",
      "are  .     So   suitcases these\n",
      "\n",
      "          go                       \n",
      "  ________|____________________     \n",
      " |   |    |       |      |  without\n",
      " |   |    |       |      |     |    \n",
      " |   |    |       |      |   seeing\n",
      " |   |    |       |      |     |    \n",
      "You can barely anywhere  .    one  \n",
      "\n",
      "          went               \n",
      "  _________|_________         \n",
      " |   |     to        |       \n",
      " |   |     |         |        \n",
      " |   |  dentist     day      \n",
      " |   |     |      ___|____    \n",
      " I   .    the   the     other\n",
      "\n",
      "                   saw                           \n",
      "  __________________|_________                    \n",
      " |   |   |    |              jump                \n",
      " |   |   |    |      _________|__________         \n",
      " |   |   |    |     |    |    |         out      \n",
      " |   |   |    |     |    |    |          |        \n",
      " |   |   |    |     |    |    |          of      \n",
      " |   |   |    |     |    |    |          |        \n",
      " |   |   |    |     |    |    |         bag      \n",
      " |   |   |    |     |    |    |          |        \n",
      " |   |   |  enough  |    |    |       dentist    \n",
      " |   |   |    |     |    |    |     _____|_____   \n",
      " ,   I   .   Sure   an angry one   my          's\n",
      "\n",
      "    noticed         \n",
      "  _____|__________   \n",
      "She  hardly even  . \n",
      "\n",
      "     goes         \n",
      "  ____|______      \n",
      " |    |   sentence\n",
      " |    |      |     \n",
      "here  !     Your  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "silly squid sentences parsed into dependency trees visually!\n",
    "\"\"\"\n",
    "    \n",
    "\n",
    "import spacy\n",
    "from nltk import Tree\n",
    "\n",
    "dependency_parser = spacy.load('en')\n",
    "squids_text = \"So many squids are jumping out of suitcases these days. You can barely go anywhere without seeing one. I went to the dentist the other day. Sure enough, I saw an angry one jump out of my dentist's bag. She hardly even noticed.\"\n",
    "parsed_squids = dependency_parser(squids_text)\n",
    "\n",
    "# Assign my_sentence a new value:\n",
    "my_sentence = \"Your sentence goes here!\"\n",
    "my_parsed_sentence = dependency_parser(my_sentence)\n",
    "\n",
    "def to_nltk_tree(node):\n",
    "  if node.n_lefts + node.n_rights > 0:\n",
    "    parsed_child_nodes = [to_nltk_tree(child) for child in node.children]\n",
    "    return Tree(node.orth_, parsed_child_nodes)\n",
    "  else:\n",
    "    return node.orth_\n",
    "\n",
    "for sent in parsed_squids.sents:\n",
    "  to_nltk_tree(sent.root).pretty_print()\n",
    "  \n",
    "for sent in my_parsed_sentence.sents:\n",
    " to_nltk_tree(sent.root).pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        jumping                \n",
      "  _________|________________    \n",
      " |   |   squids    out      |  \n",
      " |   |     |        |       |   \n",
      " |   |    many      of     days\n",
      " |   |     |        |       |   \n",
      "are  .     So   suitcases these\n",
      "\n",
      "          go                       \n",
      "  ________|____________________     \n",
      " |   |    |       |      |  without\n",
      " |   |    |       |      |     |    \n",
      " |   |    |       |      |   seeing\n",
      " |   |    |       |      |     |    \n",
      "You can barely anywhere  .    one  \n",
      "\n",
      "          went               \n",
      "  _________|_________         \n",
      " |   |     to        |       \n",
      " |   |     |         |        \n",
      " |   |  dentist     day      \n",
      " |   |     |      ___|____    \n",
      " I   .    the   the     other\n",
      "\n",
      "                   saw                           \n",
      "  __________________|_________                    \n",
      " |   |   |    |              jump                \n",
      " |   |   |    |      _________|__________         \n",
      " |   |   |    |     |    |    |         out      \n",
      " |   |   |    |     |    |    |          |        \n",
      " |   |   |    |     |    |    |          of      \n",
      " |   |   |    |     |    |    |          |        \n",
      " |   |   |    |     |    |    |         bag      \n",
      " |   |   |    |     |    |    |          |        \n",
      " |   |   |  enough  |    |    |       dentist    \n",
      " |   |   |    |     |    |    |     _____|_____   \n",
      " ,   I   .   Sure   an angry one   my          's\n",
      "\n",
      "    noticed         \n",
      "  _____|__________   \n",
      "She  hardly even  . \n",
      "\n",
      "             Within                                                               \n",
      "  _____________|________________                                                   \n",
      " |                            vault                                               \n",
      " |    __________________________|________                                          \n",
      " |   |   |     |                        left                                      \n",
      " |   |   |     |       __________________|______                                   \n",
      " |   |   |     |      |    |    |              for                                \n",
      " |   |   |     |      |    |    |               |                                  \n",
      " |   |   |     |      |    |    |            millenia                             \n",
      " |   |   |     |      |    |    |     __________|_____________                     \n",
      " |   |   |     |      |    |    |    |                     stirred                \n",
      " |   |   |     |      |    |    |    |    ____________________|___________         \n",
      " |   |   |     |      |    |    |    |   |             |               waiting    \n",
      " |   |   |     |      |    |    |    |   |             |                  |        \n",
      " |   |   |     |      |    |    |    |   |            evil              awoken    \n",
      " |   |   |     |      |    |    |    |   |       ______|______       _____|_____   \n",
      " .  the old decrepid that had  been  a   ,      an         ancient  to          be\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "adding my sentence\n",
    "\"\"\"\n",
    "    \n",
    "\n",
    "import spacy\n",
    "from nltk import Tree\n",
    "\n",
    "dependency_parser = spacy.load('en')\n",
    "squids_text = \"So many squids are jumping out of suitcases these days. You can barely go anywhere without seeing one. I went to the dentist the other day. Sure enough, I saw an angry one jump out of my dentist's bag. She hardly even noticed.\"\n",
    "parsed_squids = dependency_parser(squids_text)\n",
    "\n",
    "# Assign my_sentence a new value:\n",
    "my_sentence = \"Within the old decrepid vault that had been left for a millenia stirred an ancient evil, waiting to be awoken.\"\n",
    "my_parsed_sentence = dependency_parser(my_sentence)\n",
    "\n",
    "def to_nltk_tree(node):\n",
    "  if node.n_lefts + node.n_rights > 0:\n",
    "    parsed_child_nodes = [to_nltk_tree(child) for child in node.children]\n",
    "    return Tree(node.orth_, parsed_child_nodes)\n",
    "  else:\n",
    "    return node.orth_\n",
    "\n",
    "for sent in parsed_squids.sents:\n",
    "  to_nltk_tree(sent.root).pretty_print()\n",
    "  \n",
    "for sent in my_parsed_sentence.sents:\n",
    " to_nltk_tree(sent.root).pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
