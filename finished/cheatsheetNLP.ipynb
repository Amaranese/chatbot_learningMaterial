{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Models\n",
    "\n",
    "Language models are probabilistic machine models of language used for NLP comprehension tasks. They learn a probability of word occurrence over a sequence of words and use it to estimate the relative likelihood of different phrases. This is useful in many applications, such as speech recognition, optical character recognition, handwriting recognition, machine translation, spelling correction, and many other applications.\n",
    "\n",
    "Common language models include:\n",
    "\n",
    "## Statistical models\n",
    "\n",
    "Bag of words (unigram model)\n",
    "applications include term frequency, topic modeling, and word clouds  \n",
    "\n",
    "- n-gram models  \n",
    "- Neural Language Modeling (NLM).  \n",
    "- Natural Language Processing  \n",
    "\n",
    "Natural language processing (NLP) is concerned with enabling computers to interpret, analyze, and approximate the generation of human speech. Typically, this would refer to tasks such as generating responses to questions, translating languages, identifying languages, summarizing documents, understanding the sentiment of text, spell checking, speech recognition, and many other tasks. The field is at the intersection of linguistics, AI, and computer science.  \n",
    "\n",
    "\n",
    "## Natural Language Toolkit  \n",
    "\n",
    "Natural Language Toolkit (NLTK) is a Python library used for building Python programs that work with human language data for applying in statistical natural language processing (NLP).\n",
    "  \n",
    "  \n",
    "NLTK contains text processing libraries for tokenization, parsing, classification, stemming, tagging and semantic reasoning. It also includes graphical demonstrations and sample data sets for NLP.\n",
    "  \n",
    "  \n",
    "- Text Similarity in NLP\n",
    "- Text similarity is a facet of NLP concerned with the similarity between texts. Two popular text similarity metrics are Levenshtein distance and cosine similarity.\n",
    "\n",
    "Levenshtein distance, also called edit distance, is defined as the minimum number of edit operations (deletions, insertions, or substitutions) required to transform a text into another.\n",
    "\n",
    "Cosine similarity measures the cosine of the angle between two vectors. To determine cosine similarity, text documents need to be converted into vectors.\n",
    "\n",
    "## Language Prediction in NLP\n",
    "  \n",
    "Language prediction is an application of NLP concerned with predicting language given preceding language.\n",
    "\n",
    "Auto-suggest and suggested replies are common forms of language prediction. Common approaches inlcude:\n",
    "\n",
    "n-grams using Markov chains,\n",
    "Long Short Term Memory (LSTM) using a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Regular Expressions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional Quantifiers in Regular Expressions\n",
    "In Regular expressions, optional quantifiers are denoted by a question mark `?`. It indicates that a character can appear either 0 or 1 time. For example, the regular expression humou`?`r will match the text humour as well as the text humor.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Sets in Regular Expressions\n",
    "Regular expression character sets denoted by a pair of brackets `[]` will match any of the characters included within the brackets. For example, the regular expression con`[sc]`en`[sc]`us will match any of the spellings `consensus, concensus, consencus, and concencus.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Literals in Regular Expressions\n",
    "\n",
    "In Regular expression, the `literals` are the simplest characters that will match the exact text of the literals. For example, the regex `monkey` will completely match the text `monkey` but will also match monkey in text `The monkeys like to eat bananas.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wildcards in Regular expressions\n",
    "In Regular expression, wildcards are denoted with the period `.` and it can match any single character (letter, number, symbol or whitespace) in a piece of text. For example, the regular expression `.........` will match the text `orangutan`, `marsupial`, or any other 9-character text.\n",
    "  \n",
    "## Regular Expression Ranges\n",
    "Regular expression ranges are used to specify a range of characters that can be matched. Common regular expression ranges include:   \n",
    "`[A-Z]`. : match any uppercase letter    \n",
    "`[a-z].` : match any lowercase letter    \n",
    "`[0-9]`. : match any digit    \n",
    "`[A-Za-z]` : match any uppercase or lowercase letter.  \n",
    "  \n",
    "## Shorthand Character Classes in Regular Expressions\n",
    "Shorthand character classes simplify writing regular expressions. For example, \\w represents the regex range [A-Za-z0-9_], \\d represents [0-9], \\W represents [^A-Za-z0-9_] matching any character not included by \\w, \\D represents [^0-9] matching any character not included by \\d.\n",
    "  \n",
    "## Kleene Star & Kleene Plus in Regular Expressions\n",
    "In Regular expressions, the Kleene star(*) indicates that the preceding character can occur 0 or more times. For example, meo*w will match mew, meow, meooow, and meoooooooooooow. The Kleene plus(+) indicates that the preceding character can occur 1 or more times. For example, meo+w will match meow, meooow, and meoooooooooooow, but not match mew.\n",
    "  \n",
    "## Grouping in Regular Expressions\n",
    "In Regular expressions, grouping is accomplished by open `( and close parenthesis )`. Thus the regular expression I love `(baboons|gorillas)` will match the text `I love baboons` as well as `I love gorillas`, as the grouping limits the reach of the | to the text within the parentheses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Text Preprocessing\n",
    "In natural language processing, text preprocessing is the practice of cleaning and preparing text data. NLTK and re are common Python libraries used to handle many text preprocessing tasks.\n",
    "  \n",
    "## Noise Removal  \n",
    "\n",
    "In natural language processing, noise removal is a text preprocessing task devoted to stripping text of formatting.\n",
    "\n",
    "```python\n",
    "import re\n",
    " \n",
    "text = \"Five fantastic fish flew off to find faraway functions. Maybe find another five fantastic fish? Find my fish with a function please!\"\n",
    " \n",
    "# remove punctuation\n",
    "result = re.sub(r'[\\.\\?\\!\\,\\:\\;\\\"]', '', text)\n",
    " \n",
    "print(result)\n",
    "# Five fantastic fish flew off to find faraway functions Maybe find another five fantastic fish Find my fish with a function please\n",
    "```\n",
    "  \n",
    "## Tokenization\n",
    "In natural language processing, tokenization is the text preprocessing task of breaking up text into smaller components of text (known as tokens).    \n",
    "\n",
    "```python\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "text = \"This is a text to tokenize\"\n",
    "tokenized = word_tokenize(text)\n",
    " \n",
    "print(tokenized)\n",
    "# [\"This\", \"is\", \"a\", \"text\", \"to\", \"tokenize\"]\n",
    "```\n",
    "\n",
    "  \n",
    "## Text Normalization  \n",
    "\n",
    "In natural language processing, normalization encompasses many text preprocessing tasks including stemming, lemmatization, upper or lowercasing, and stopwords removal.  \n",
    "  \n",
    "## Stemming\n",
    "In natural language processing, stemming is the text preprocessing normalization task concerned with bluntly removing word affixes (prefixes and suffixes).    \n",
    "\n",
    "```python\n",
    "from nltk.stem import PorterStemmer\n",
    " \n",
    "tokenized = [\"So\", \"many\", \"squids\", \"are\", \"jumping\"]\n",
    " \n",
    "stemmer = PorterStemmer()\n",
    "stemmed = [stemmer.stem(token) for token in tokenized]\n",
    " \n",
    "print(stemmed)\n",
    "# ['So', 'mani', 'squid', 'are', 'jump']\n",
    "\n",
    "```\n",
    "  \n",
    "## Lemmatization  \n",
    "\n",
    "In natural language processing, lemmatization is the text preprocessing normalization task concerned with bringing words down to their root forms. \n",
    "\n",
    "```Python\n",
    "from nltk.stem import WordNetLemmatizer\n",
    " \n",
    "tokenized = [\"So\", \"many\", \"squids\", \"are\", \"jumping\"]\n",
    " \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(token) for token in tokenized]\n",
    " \n",
    "print(stemmed)\n",
    "# ['So', 'many', 'squid', 'be', 'jump']\n",
    "\n",
    "```\n",
    "\n",
    "## Stopword Removal  \n",
    "\n",
    "In natural language processing, stopword removal is the process of removing words from a string that donâ€™t provide any information about the tone of a statement.  \n",
    "  \n",
    "```python\n",
    "from nltk.corpus import stopwords \n",
    " \n",
    "# define set of English stopwords\n",
    "stop_words = set(stopwords.words('english')) \n",
    " \n",
    "# remove stopwords from tokens in dataset\n",
    "statement_no_stop = [word for word in word_tokens if word not in stop_words]\n",
    "```\n",
    "\n",
    "## Part-of-Speech Tagging\n",
    "\n",
    "In natural language processing, part-of-speech tagging is the process of assigning a part of speech to every word in a string. Using the part of speech can improve the results of lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule-Based Chatbots\n",
    "\n",
    "\n",
    "## Rule-Based Chatbots\n",
    "Rule-based chatbots are structured as a dialog tree and often use regular expressions to match a userâ€™s input to human-like responses. The aim is to simulate the back-and-forth of a real-life conversation, often in a specific context, like telling the user what the weather is like outside. In chatbot design, rule-based chatbots are closed-domain, also called dialog agents, because they are limited to conversations on a specific subject.\n",
    "\n",
    "## Chatbot Intents\n",
    "In chatbots design, an intent is the purpose or category of the user query. The userâ€™s utterance gets matched to a chatbot intent. In rule-based chatbots, you can use regular expressions to match a userâ€™s statement to a chatbot intent.\n",
    "\n",
    "```python\n",
    "import re\n",
    " \n",
    "matching_intents = {'weather_intent': [r'weather.*on (\\w+)']}\n",
    " \n",
    "def match_reply(self, reply):\n",
    "  for key, values in matching_intents.items():\n",
    "    for regex_pattern in values:\n",
    "      found_match = re.match(regex_pattern, reply.lower())\n",
    "      if found_match and key == 'weather_intent':\n",
    "        return weather_intent(found_match.groups()[0])\n",
    "        \n",
    "  return input(\"I did not understand you. Can you please ask your question again?\")\n",
    "```\n",
    "\n",
    "## Chatbot Utterances\n",
    "In chatbot design, an utterance is a statement that the user makes to the chatbot. The chatbot attempts to match the utterance to an intent.\n",
    "   \n",
    "## Chatbot Entities\n",
    "In chatbot design, an entity is a value that is parsed from a user utterance and passed for use within the user response.  \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
